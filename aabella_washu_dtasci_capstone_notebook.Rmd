---
title: 'Coursera WashU Dtasci Capstone Project: BLIGHT'
always_allow_html: yes
output:
  html_notebook: default
---

Loading libraries
```{r, include=FALSE}
library(readr)
library(magrittr)
library(tidyr)
library(dplyr)
library(fuzzyjoin)
#install.packages("lucr")
library(lucr)
#install.packages("lubridate")
library(lubridate)
#install.packages("rbokeh")
library(rbokeh)
#install.packages("geosphere")
library(geosphere)
#install.packages("caret")
library(caret)
```

Setting data location, make sure the files below are available so the rest runs
```{r}
datadir <- "../data"
list.files(datadir)
```

Loading data permits -- mainly blight incidents
```{r}
data_permits <- read_tsv(paste(datadir, "detroit-demolition-permits.tsv", sep="/"))
head(data_permits)
```

Data permits exploration
```{r}
#converting to factors
cols <- c("CASE_TYPE", "CASE_DESCRIPTION", "LEGAL_USE", "BLD_PERMIT_TYPE",
          "PERMIT_DESCRIPTION", "BLD_PERMIT_DESC", "BLD_TYPE_USE", "RESIDENTIAL",
          "DESCRIPTION", "BLD_TYPE_CONST_COD", "BLD_ZONING_DIST", "BLD_USE_GROUP",
          "BLD_BASEMENT", "FEE_TYPE", "CSF_CREATED_BY","CONDITION_FOR_APPROVAL")
data_permits %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("PCF_AMT_PD", "PCF_AMT_DUE", "PCF_UPDATED","ESTIMATED_COST")
data_permits %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("PERMIT_APPLIED","PERMIT_ISSUED","PERMIT_EXPIRES")
# to use lubridate, need to figure it out -- 
data_permits %<>% mutate_each_(funs(parse_date_time(.,orders="mdy",tz="America/Detroit")),cols)
summary(data_permits)
```
Extracting building lat longs
```{r}
data_permits %<>%
  #filter out permits that have no lat/long
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",site_location)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", site_location))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", site_location))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", site_location))

```

Create a list of buildings doing some magic to remove those entries whose lat/long stdev is larger than 10e-4 (~11m). Not much gets removed actually, but it's consistent with steps below. Removed those records whose address was a lat/long (only ~40).
```{r}
bld_list_permit <- data_permits %>%
  mutate(r = sqrt(PARCEL_SIZE/pi) ) %>%
  select(address=address_only, PARCEL_NO, LOT_NUMBER, PERMIT_ISSUED, PARCEL_SIZE, lat, long, r) %>%
  filter(! grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",address)) %>%
  arrange(address, desc(PERMIT_ISSUED)) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  arrange(PERMIT_ISSUED) %>%
  summarise(n_permits=n(), last_permit=last(PERMIT_ISSUED), 
            lat=median(lat), long=median(long), r=last(r))
head(bld_list_permit)
```

Plot lat/longs to a map
- centering to Detroit 42.3314 N, 83.0458 W
- more about rbokeh: http://hafen.github.io/rbokeh/rd.html#gmap
- lots of cool maps: https://snazzymaps.com 
- Available styles are: “subtle_grayscale”, “shades_of_grey”, “blue_water”, “pale_dawn”, “blue_essence”, “apple_mapsesque”, “midnight_commander”, “light_monochrome”, “paper”, “retro”, “flat_map”, “cool_grey”
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = bld_list_permit, hover = c(address, r), 
            col = 'purple', alpha = 0.1) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```

Testing GeoSphere package to calculate distance between 2 lat/long coordinates
```{r}
#this should be around 1160ft according to Google Maps
#https://www.google.com/maps/dir/42.3655979,-71.1040001/42.3683169,-71.1017685/@42.3668028,-71.10503,17z/data=!4m2!4m1!3e2
m_in_ft <- 0.3048
distGeo(c(-71.1040001,42.3655979), c(-71.1017685,42.3683169))/m_in_ft
```

Create a function to output the buildings that are within proximity of lat/lon coordinates
```{r}
lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]
rangeft <- 1500
bld_list_permit %>% 
#  mutate(dt = distGeo(c(ln,lt), c(long, lat))/m_in_ft) %>%
  filter(!is.na(r)) %>%
  rowwise() %>%
  mutate(dt = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
  filter(dt <= rangeft + r) %>%
  head(10)

#distGeo(c(ln,lt),c(bld_list$long,bld_list$lat))
```
Function to return number of buildings around a particular lat long within a predefined range (by default 500m or 1640ft)
- input data, for simplicity, should have 4 fields
  - PARCEL_NO: assumed it's a unique identifier for building)
  - lat: building lat coordinate
  - long: building long coordinate
  - r: estimated building's area radious (when area is approximated to circle)
- the other input variables are:
  - lt: target lat coordinate
  - ln: target long coordinate
  - rangeft (optional, range in feet to look buildings around target lat/long)
  - m_in_ft (option)
```{r}
bld_in_range <- function(lt, ln, data, rangeft = 1640, m_in_ft = 0.3048) {
  data %>% 
    #filter(!is.na(r)) %>%
    rowwise() %>%
    mutate(dist_ft = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
    filter(dist_ft <= rangeft + r) %>%
    #select(address, dist_ft) %>%
    #arrange(dist_ft)
    nrow()
}

indata <- bld_list_permit %>% 
  filter(!is.na(r)) %>%
  unique() %>%
  select(address, lat, long, r)

lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]

bld_in_range(lt, ln, indata, rangeft = 500)
```
Loading blight violation incidents
```{r}
data_violations <- read_csv(paste(datadir, "detroit-blight-violations.csv", sep="/"))
head(data_violations)
```

Data Violation exploration
```{r}
#converting to factors
cols <- c("AgencyName","ViolationCode","Disposition","PaymentStatus","Void",
          "ViolationCategory","Country")
data_violations %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("FineAmt","AdminFee","LateFee","StateFee","CleanUpCost","JudgmentAmt")
data_violations %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("TicketIssuedDT","HearingDT")
# to use lubridate, need to figure it out -- 
#data_violations %<>% mutate_each_(funs(from_currency(.)),cols)
summary(data_violations)
#dplyr::glimpse(data_violations)
```

Getting the violation codes
Need to categorize them
```{r}
violCodes <- data_violations %>% 
  select(ViolationCode, ViolDescription) %>%
  unique()
```

Generated lat/longs and address, and cleaned data by keeping records with Detroit addresses only. Didn't filter by country as it removed most of the entries (from 300k to 13k records). Didn't remove disposition "not reposonsible" or "pending" as this could contain information
```{r}
viol_list <- data_violations %>% 
#  filter(Country == "US") %>%
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",ViolationAddress)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", ViolationAddress))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", ViolationAddress))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", ViolationAddress)) %>%
  filter(grepl("Detroit",ViolationAddress)) %>%
#  filter(! grepl("Not responsible",Disposition)) %>%
#  filter(! grepl("PENDING", Disposition)) %>%
  select(lat, long, ViolationCode, Disposition, JudgmentAmt, PaymentStatus, ViolationCategory, address_only) 

head(viol_list)
```

What happens with Disposition?
```{r}
viol_list %>%
  select(Disposition) %>%
  group_by(Disposition) %>%
  summarise(n())
```

There are about 80k unique lat/longs, some of them generate a huge amount of violations, here are the top 30 lat/longs
```{r}
top30viols <- viol_list %>%
  select(lat, long) %>%
  mutate(geocord = paste(lat,long)) %>%
  group_by(geocord) %>%
  summarize(lat=last(lat), long=last(long), num_viols_in_geo = n()) %>%
  arrange(desc(num_viols_in_geo)) %>%
  head(30)
top30viols
```

Plotting them, we see that the top one has 21k violations and it's in the center of Detroit, probably a standard lat/long coordinate when not the actual is not available. I'm not sure about the others with over 1000 violations... 
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = top30viols, hover = num_viols_in_geo, 
            col = 'red', alpha = pmin(num_viols_in_geo / 1000, 1)) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```


Are there the same number of unique addresses?
```{r}
viol_list %>%
  select(address_only) %>%
  group_by(address_only) %>%
  summarize(num_viols_in_address = n()) %>%
  arrange(desc(num_viols_in_address))
```

Well, it seems that there are about 110k unique addresses, and 73k unique lat/longs. Let's see how they contrast in terms of number of violations (lat/long vs. address)
```{r}
viol_list_cleaned <- viol_list %>%
  mutate(geocoord = paste(lat,long)) %>%
  group_by(geocoord) %>%
  mutate(num_viols_in_geocoord = n())

viol_list_cleaned %<>% 
  group_by(address_only) %>%
  mutate(num_viols_in_address = n())

head(viol_list_cleaned)
```

It seems that about 70k entries have different number of violations when looking by address or lat/long, being 35k unique records duplicated, so getting rid of them
```{r}
viol_list_cleaned %<>%
  filter(! num_viols_in_address != num_viols_in_geocoord) %>%
  group_by(ViolationCode, address_only) %>%
  mutate(num_viols_by_vcode = n()) %>%
  arrange(desc(num_viols_in_geocoord)) %>%
  ungroup() %>%
  unique() 

head(viol_list_cleaned)
```

Getting a list of buildings, the grouping wouldn't be necessary as there is a one to one lat/long to address correspondance, but leaving it just in case. 
```{r}
bld_list_viol <- viol_list_cleaned %>%
  select(address=address_only, lat, long, ViolationCode, Disposition, 
         JudgmentAmt, PaymentStatus) %>%
  group_by(address, Disposition) %>%
  mutate(num_disposition = n()) %>% 
  group_by(address) %>%
  mutate(num_viols = n(), max_amt = max(JudgmentAmt)) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter(grepl("^Responsible", Disposition)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarise(lat=median(lat), long=median(long), num_viols = last(num_viols), 
            num_responsible = last(num_disposition), max_amt =last(max_amt)) %>%
  unique() 
#  ungroup() %>%
#  group_by(address) %>%
#  rowwise() %>%
#  mutate(num_responsible = ifelse(grepl("^Responsible", Disposition)>0,num_disposition,0)) %>%

head(bld_list_viol)
```

Loading calls to 311, typically complains
```{r}
data_311 <- read_csv(paste(datadir, "detroit-311.csv", sep="/"))

#converting to factors
cols <- c("issue_type", "ticket_status")
data_311 %<>% mutate_each_(funs(factor(.)),cols)

#converting to dates
cols <-c("ticket_closed_date_time", "acknowledged_at", "ticket_created_date_time",
         "ticket_last_updated_date_time")
data_311 %<>% mutate_each_(funs(parse_date_time(.,orders="mdY HMS Op",tz="America/Detroit")),cols)

#dplyr::glimpse(data_311)
summary(data_311)
```
There are 23 types of 311 issues, we could extract counts for each
```{r}
data_311 %>%
  group_by(issue_type) %>%
  summarise(num_311type = n()) %>%
  arrange(desc(num_311type))
```

Most tickets are either archived or closed, probably there is no discrimination around that
```{r}
data_311 %>%
  group_by(ticket_status) %>%
  summarise(num_tx_status = n()) %>%
  arrange(desc(num_tx_status))
```
Rating could be interesting to use
```{r}
data_311 %>%
  group_by(rating) %>%
  summarise(num_rating = n()) %>%
  arrange(desc(num_rating))
```

All entries are for city of Detroit, which is good, but won't be discriminating
```{r}
data_311 %>%
  group_by(city) %>%
  summarise(num_city = n()) %>%
  arrange(desc(num_city))
```

Grouping by address, it seems there are multiple lat/longs per address, removing those entries that have a lat/long STDEV larger than 10e-4 (~11m)
```{r}
bld_list_311 <- data_311 %>%
  select(address, lat, long=lng, rating, type311 = issue_type, rating) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarize(lat=median(lat), long=median(long), num_311 = n(), max_rating = max(rating), min_rating=min(rating), diff_rating = max(rating) - min(rating)) %>%
  unique()

head(bld_list_311)
```
Most 311 entries don't change the rating
```{r}
bld_list_311 %>%
  group_by(diff_rating) %>%
  tally(sort=TRUE)
```

Loading criminal incidents in Detroit
```{r}
data_crime <- read_csv(paste(datadir, "detroit-crime.csv", sep="/"))

#converting to factors
cols <- c("CATEGORY","STATEOFFENSEFILECLASS","PRECINCT","COUNCIL","NEIGHBORHOOD")
data_crime %<>% mutate_each_(funs(factor(.)),cols)

#converting to dates
cols <-c("INCIDENTDATE")
data_crime %<>% mutate_each_(funs(parse_date_time(.,orders="mdY HMS Op",tz="America/Detroit")),cols)

#dplyr::glimpse(data_crime)
summary(data_crime)
```
There are 50 categories of crimes, could be added as counts to the features
```{r}
data_crime %>%
  group_by(CATEGORY) %>%
  tally(sort=TRUE)
```

Seems that crime is spread evenly, except for districts 3 and 5
```{r}
data_crime %>% 
  group_by(COUNCIL) %>%
  tally(sort=TRUE)
```

Grouping by address, it seems there are multiple lat/longs per address, and about 1000 entries have a lat/long STDEV larger than 10e-4 (~11m distance), so I removed those
```{r}
bld_list_crime <- data_crime %>%
  select(address=ADDRESS, lat=LAT, long=LON, typecrime = CATEGORY) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarize(lat=median(lat), long=median(long), num_crime = n()) %>%
  unique()

head(bld_list_crime)
```
Let's join all lat/longs and reduce their precision to 4 digits (~11m accuracy)
```{r}
bld_list_coord <- bld_list_311 %>%
  select(lat, long) %>%
  bind_rows(select(bld_list_crime, lat, long)) %>%
  bind_rows(select(bld_list_viol, lat, long)) %>%
  transmute(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  distinct() %>%
  arrange(desc(lat), desc(long))
  
bld_list_coord
```
Let's reduce the precision to 4 decimal points (~11m in the equator)
```{r}
bld_list_311 %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))

bld_list_crime %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))

bld_list_viol %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))
```

Direct join
```{r}
#bld_list_crime_311_geosmart <- bld_list_crime %>% 
#  head(100) %>%
#  geo_join(bld_list_311, by=c("lat", "long"),  max_dist=0.1, unit="km", mode="inner")

bld_list_crime_311_viol <- bld_list_crime %>%
  full_join(bld_list_311, by = "coord") %>%
#  select(-address.x, -address.y) %>% 
  full_join(bld_list_viol, by = "coord") %>%
  select(coord, num_crime, num_311, max_rating, min_rating, diff_rating, num_viols, num_responsible, max_amt)
```

Taking care of NA's, doing some dummy imputation
```{r}
cols <- c("num_crime", "num_311", "num_viols", "num_responsible")
bld_list_crime_311_viol %<>% mutate_each_(funs(ifelse(is.na(.),0,.)),cols)

cols <- c("max_rating", "min_rating", "diff_rating", "max_amt")
bld_list_crime_311_viol %<>% mutate_each_(funs(ifelse(is.na(.),-1,.)),cols)

bld_list_crime_311_viol %<>% separate(coord, c("lat","long"), sep=" ", remove=FALSE) %>%
  mutate(lat = as.double(lat), long = as.double(long))

bld_list_crime_311_viol %<>% filter(! is.na(lat))
```

Speeding up version by looking computing distance directly in degrees, 0.0001 ~ 11m ~ 37ft
```{r}
in_blight <- function(lt, ln, data) {
  data %>% 
    filter(sqrt((lat-lt)^2+(long-ln)^2)<rdegr+0.0001) %>%
    nrow()
}

indata_degrees <- indata %>%
  mutate(rdegr = 0.0001/37 * r)
```


The mashup results in about 8523 blighted entries, out of 144k total records. 2k blighted entries are potentially overlaps due to lat/long matching
```{r}
modeling_data <- bld_list_crime_311_viol 

modeling_data %<>%
  rowwise() %>%
  mutate(nblighted = in_blight(lat, long, indata_degrees)) 

modeling_data %>%
  filter(nblighted > 0) %>% 
  nrow()
```

Creating dependent variable "condition" as boolean from "nblighted" (this describes the number of records blighted in the same lat/long)
```{r}
modeling_data %<>%
  mutate(condition = factor(if_else(nblighted > 0, "BLIGHTED", "NOT_BLIGHTED"))) %>%
  select(-coord, -lat, -long, -nblighted)

table(modeling_data$condition)
```
Balancing the two classes by downsampling the number of cases of non-blight (not ideal, but helps modeling and compute time)
```{r}
mod_data_sampled <- modeling_data %>%
  filter(condition == "BLIGHTED")

mod_data_sampled <- modeling_data %>%
  filter(condition == "NOT_BLIGHTED") %>% 
  sample_n(9000) %>%
  bind_rows(mod_data_sampled)

table(mod_data_sampled$condition)
```
Creating a set-aside dataset for test, and a modeling set
```{r}
set.seed(107)
inTest <- createDataPartition(y=mod_data_sampled$condition, p=0.1, list=FALSE)
testSet <- mod_data_sampled[inTest,]
modelSet <- mod_data_sampled[-inTest,]
table(testSet$condition)
```
Creating a training and validation data sets for model training and validation, the training set is about 20k records
```{r}
set.seed(107)
inValid <- createDataPartition(y=modelSet$condition, p=0.15, list=FALSE)
trainSet <- mod_data_sampled[-inValid,]
validSet <- mod_data_sampled[inValid,]
table(trainSet$condition)
```
As for the validation set, it is about 3k records
```{r}
table(validSet$condition)
```
Plotting the histogram of number of violations for both blighted and not blighted buildings, we see some difference that might contribute to discriminate the 2 classes
```{r}
p <- figure(width = 600, height = 350) %>% 
  ly_hist(num_viols, data = trainSet[which(trainSet$condition == "NOT_BLIGHTED"),], color = "blue", alpha = 0.25, freq = F, lname = "NOT BLIGHTED") %>%
  ly_hist(num_viols, data = trainSet[which(trainSet$condition == "BLIGHTED"),], color = "red", alpha = 0.25, freq = F, lname = "BLIGHTED")
p
```
Let's train a general linear model (GLM) with 5-fold cross-validation trying to predict the bligth condition only by the number of violations reported in the coordinate ("num_viols_in_geocoord", calculated before). The performance is quite low. 
```{r}
# 5 fold cross-validation, 1 repetition
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 1,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )

# general linear model
glmModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "glm",
                  trControl = ctrl,
                  metric = "ROC"
                  )
glmModel
```
Let's check the results in the validation set
```{r}
glmVal <- predict(glmModel, newdata = validSet)
confusionMatrix(glmVal, validSet$condition, positive = "BLIGHTED")
```

Let's try a decision tree
```{r}
# decision tree model
dtreeModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "rpart",
                  trControl = ctrl,
                  metric = "ROC"
                  )
dtreeModel
```

Let's check the results in the validation set
```{r}
dtreeVal <- predict(dtreeModel, newdata = validSet)
confusionMatrix(dtreeVal, validSet$condition, positive = "BLIGHTED")
```

Let's try a random forest
```{r}
# random forest model
rfModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel
```
Let's check the results in the training set
```{r}
rfVal <- predict(rfModel, newdata = trainSet)
confusionMatrix(rfVal, trainSet$condition, positive = "BLIGHTED")
```

Let's check the results in the validation set
```{r}
rfVal <- predict(rfModel, newdata = validSet)
confusionMatrix(rfVal, validSet$condition, positive = "BLIGHTED")
```
Let's compare the models across cross-validation resamples
```{r}
resamps <- resamples(list(glm = glmModel, dtree = dtreeModel, rf = rfModel))
summary(resamps)
```

Let's add more features in the dtree
```{r}
# decision tree model
dtreeModel2 <- train(condition ~ .,
                  data = trainSet,
                  method = "rpart",
                  trControl = ctrl,
                  metric = "ROC",
                  na.action=na.exclude
                  )
dtreeModel2
```

Let's check the results in the validation set
```{r}
dtree2Val <- predict(dtreeModel2, newdata = validSet)
confusionMatrix(rfVal, validSet$condition, positive = "BLIGHTED")
```
Let's try a random forest with all the variables
```{r}
# random forest model
rfModel2 <- train(condition ~ .,
                  data = trainSet,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel2
```
```{r}
rf2Val <- predict(rfModel2, newdata = validSet)
confusionMatrix(rfVal, validSet$condition, positive = "BLIGHTED")
```
```{r}
rf2varImp <- varImp(rfModel2, scale = FALSE)
plot(rf2varImp, top=8)
```

Next steps:
- why 311 doesn't have much effect? Maybe not enough representation, how many 311 records are in modeling data? And others?
- investigate crime/311/viol type counts as features (like bagofwords)
- 311: investigate effect of time (e.g., between created and closed, or # tx / unit of time)
