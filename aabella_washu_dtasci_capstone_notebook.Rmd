---
title: 'Coursera WashU Dtasci Capstone Project: BLIGHT'
always_allow_html: yes
output:
  html_notebook: default
---

Loading libraries
```{r, include=FALSE}
library(readr)
library(magrittr)
library(tidyr)
library(dplyr)
library(fuzzyjoin)
#install.packages("lucr")
library(lucr)
#install.packages("lubridate")
library(lubridate)
#install.packages("rbokeh")
library(rbokeh)
#install.packages("geosphere")
library(geosphere)
#install.packages("caret")
library(caret)
```

Setting data location, make sure the files below are available so the rest runs
```{r}
datadir <- "../data"
list.files(datadir)
```

Loading data permits -- mainly blight incidents
```{r}
data_permits <- read_tsv(paste(datadir, "detroit-demolition-permits.tsv", sep="/"))
head(data_permits)
```

Data permits exploration
```{r}
#converting to factors
cols <- c("CASE_TYPE", "CASE_DESCRIPTION", "LEGAL_USE", "BLD_PERMIT_TYPE",
          "PERMIT_DESCRIPTION", "BLD_PERMIT_DESC", "BLD_TYPE_USE", "RESIDENTIAL",
          "DESCRIPTION", "BLD_TYPE_CONST_COD", "BLD_ZONING_DIST", "BLD_USE_GROUP",
          "BLD_BASEMENT", "FEE_TYPE", "CSF_CREATED_BY","CONDITION_FOR_APPROVAL")
data_permits %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("PCF_AMT_PD", "PCF_AMT_DUE", "PCF_UPDATED","ESTIMATED_COST")
data_permits %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("PERMIT_APPLIED","PERMIT_ISSUED","PERMIT_EXPIRES")
# to use lubridate, need to figure it out -- 
data_permits %<>% mutate_each_(funs(parse_date_time(.,orders="mdy",tz="America/Detroit")),cols)
summary(data_permits)
```
Extracting building lat longs
```{r}
data_permits %<>%
  #filter out permits that have no lat/long
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",site_location)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", site_location))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", site_location))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", site_location))

```

Create a list of buildings doing some magic to remove those entries whose lat/long stdev is larger than 10e-4 (~11m). Not much gets removed actually, but it's consistent with steps below. Removed those records whose address was a lat/long (only ~40).
```{r}
bld_list_permit <- data_permits %>%
  mutate(r = sqrt(PARCEL_SIZE/pi) ) %>%
  select(address=address_only, PARCEL_NO, LOT_NUMBER, PERMIT_ISSUED, PARCEL_SIZE, lat, long, r) %>%
  filter(! grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",address)) %>%
  arrange(address, desc(PERMIT_ISSUED)) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  arrange(PERMIT_ISSUED) %>%
  summarise(n_permits=n(), last_permit=last(PERMIT_ISSUED), 
            lat=median(lat), long=median(long), r=last(r))
head(bld_list_permit)
```

Plot lat/longs to a map
- centering to Detroit 42.3314 N, 83.0458 W
- more about rbokeh: http://hafen.github.io/rbokeh/rd.html#gmap
- lots of cool maps: https://snazzymaps.com 
- Available styles are: “subtle_grayscale”, “shades_of_grey”, “blue_water”, “pale_dawn”, “blue_essence”, “apple_mapsesque”, “midnight_commander”, “light_monochrome”, “paper”, “retro”, “flat_map”, “cool_grey”
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = bld_list_permit, hover = c(address, r), 
            col = 'purple', alpha = 0.1) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```

Testing GeoSphere package to calculate distance between 2 lat/long coordinates
```{r}
#this should be around 1160ft according to Google Maps
#https://www.google.com/maps/dir/42.3655979,-71.1040001/42.3683169,-71.1017685/@42.3668028,-71.10503,17z/data=!4m2!4m1!3e2
m_in_ft <- 0.3048
distGeo(c(-71.1040001,42.3655979), c(-71.1017685,42.3683169))/m_in_ft
```

Create a function to output the buildings that are within proximity of lat/lon coordinates
```{r}
lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]
rangeft <- 1500
bld_list_permit %>% 
#  mutate(dt = distGeo(c(ln,lt), c(long, lat))/m_in_ft) %>%
  filter(!is.na(r)) %>%
  rowwise() %>%
  mutate(dt = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
  filter(dt <= rangeft + r) %>%
  head(10)

#distGeo(c(ln,lt),c(bld_list$long,bld_list$lat))
```
Function to return number of buildings around a particular lat long within a predefined range (by default 500m or 1640ft)
- input data, for simplicity, should have 4 fields
  - PARCEL_NO: assumed it's a unique identifier for building)
  - lat: building lat coordinate
  - long: building long coordinate
  - r: estimated building's area radious (when area is approximated to circle)
- the other input variables are:
  - lt: target lat coordinate
  - ln: target long coordinate
  - rangeft (optional, range in feet to look buildings around target lat/long)
  - m_in_ft (option)
```{r}
bld_in_range <- function(lt, ln, data, rangeft = 1640, m_in_ft = 0.3048) {
  data %>% 
    #filter(!is.na(r)) %>%
    rowwise() %>%
    mutate(dist_ft = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
    filter(dist_ft <= rangeft + r) %>%
    #select(address, dist_ft) %>%
    #arrange(dist_ft)
    nrow()
}

indata <- bld_list_permit %>% 
  filter(!is.na(r)) %>%
  unique() %>%
  select(address, lat, long, r)

lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]

bld_in_range(lt, ln, indata, rangeft = 500)
```
Loading blight violation incidents
```{r}
data_violations <- read_csv(paste(datadir, "detroit-blight-violations.csv", sep="/"))
head(data_violations)
```

Data Violation exploration
```{r}
#converting to factors
cols <- c("AgencyName","ViolationCode","Disposition","PaymentStatus","Void",
          "ViolationCategory","Country")
data_violations %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("FineAmt","AdminFee","LateFee","StateFee","CleanUpCost","JudgmentAmt")
data_violations %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("TicketIssuedDT","HearingDT")
# to use lubridate, need to figure it out -- 
#data_violations %<>% mutate_each_(funs(from_currency(.)),cols)
summary(data_violations)
#dplyr::glimpse(data_violations)
```

Getting the violation codes
Need to categorize them
```{r}
violCodes <- data_violations %>% 
  select(ViolationCode, ViolDescription) %>%
  unique()
```

Generated lat/longs and address, and cleaned data by keeping records with Detroit addresses only. Didn't filter by country as it removed most of the entries (from 300k to 13k records). Didn't remove disposition "not reposonsible" or "pending" as this could contain information
```{r}
viol_list <- data_violations %>% 
#  filter(Country == "US") %>%
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",ViolationAddress)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", ViolationAddress))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", ViolationAddress))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", ViolationAddress)) %>%
  filter(grepl("Detroit",ViolationAddress)) %>%
#  filter(! grepl("Not responsible",Disposition)) %>%
#  filter(! grepl("PENDING", Disposition)) %>%
  select(lat, long, ViolationCode, Disposition, JudgmentAmt, PaymentStatus, ViolationCategory, address_only) 

head(viol_list)
```

What happens with Disposition?
```{r}
viol_list %>%
  select(Disposition) %>%
  group_by(Disposition) %>%
  summarise(n())
```

There are about 80k unique lat/longs, some of them generate a huge amount of violations, here are the top 30 lat/longs
```{r}
top30viols <- viol_list %>%
  select(lat, long) %>%
  mutate(geocord = paste(lat,long)) %>%
  group_by(geocord) %>%
  summarize(lat=last(lat), long=last(long), num_viols_in_geo = n()) %>%
  arrange(desc(num_viols_in_geo)) %>%
  head(30)
top30viols
```

Plotting them, we see that the top one has 21k violations and it's in the center of Detroit, probably a standard lat/long coordinate when not the actual is not available. I'm not sure about the others with over 1000 violations... 
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = top30viols, hover = num_viols_in_geo, 
            col = 'red', alpha = pmin(num_viols_in_geo / 1000, 1)) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```


Are there the same number of unique addresses?
```{r}
viol_list %>%
  select(address_only) %>%
  group_by(address_only) %>%
  summarize(num_viols_in_address = n()) %>%
  arrange(desc(num_viols_in_address))
```

Well, it seems that there are about 110k unique addresses, and 73k unique lat/longs. Let's see how they contrast in terms of number of violations (lat/long vs. address)
```{r}
viol_list_cleaned <- viol_list %>%
  mutate(geocoord = paste(lat,long)) %>%
  group_by(geocoord) %>%
  mutate(num_viols_in_geocoord = n())

viol_list_cleaned %<>% 
  group_by(address_only) %>%
  mutate(num_viols_in_address = n())

head(viol_list_cleaned)
```

It seems that about 70k entries have different number of violations when looking by address or lat/long, being 35k unique records duplicated, so getting rid of them
```{r}
viol_list_cleaned %<>%
  filter(! num_viols_in_address != num_viols_in_geocoord) %>%
  group_by(ViolationCode, address_only) %>%
  mutate(num_viols_by_vcode = n()) %>%
  arrange(desc(num_viols_in_geocoord)) %>%
  ungroup() %>%
  unique() 

head(viol_list_cleaned)
```
Looking at violation codes, it seems there is some common ones, taking the top 20
```{r}
violCodes <- viol_list_cleaned %>% 
#  mutate(ViolationCode = sub("^([0-9]+-[0-9]+)-.*$","\\1",ViolationCode)) %>%
  group_by(ViolationCode) %>%
  tally(sort=TRUE) 

violCodes
```
Manually categorized 
```{r}
violCodes_manual_categorization <-
  read_csv("violCodes_manual_categorization.csv")

violCodes_manual_categorization %<>%
  mutate(ViolGroup=as.factor(ViolGroup),
         ViolationCode=as.factor(ViolationCode))

violCodes_manual_categorization
```
Adding a grouping factor for violation categories, ViolGroup
```{r}
viol_list_cleaned %<>% 
  left_join(violCodes_manual_categorization,by="ViolationCode")

#Some codes had no description, to prevent to NAs, grouping them to other
viol_list_cleaned[which(is.na(viol_list_cleaned$ViolGroup)),]$ViolGroup <- "other"

#viol_list_cleaned %<>%
#  mutate(ViolGroup = ifelse(is.na(ViolGroup),"other",ViolGroup))

head(viol_list_cleaned)
```


Expand ViolGroup counts as separate features
```{r}
violcodes_counts <- viol_list_cleaned %>%
  select(address=address_only, lat, long, ViolGroup, Disposition, 
         JudgmentAmt, PaymentStatus) %>%
  group_by(address, ViolGroup) %>%
  summarize(num_viol_by_code = n()) %>%
  ungroup() %>%
  spread(ViolGroup, num_viol_by_code, fill = 0)

head(violcodes_counts)
```

Getting a list of buildings, the grouping wouldn't be necessary as there is a one to one lat/long to address correspondance, but leaving it just in case. 
```{r}
bld_list_viol <- viol_list_cleaned %>%
  select(address=address_only, lat, long, ViolationCode, Disposition, 
         JudgmentAmt, PaymentStatus) %>%
  group_by(address, ViolationCode) %>%
  group_by(address, Disposition) %>%
  mutate(num_disposition = n()) %>% 
  group_by(address) %>%
  mutate(num_viols = n(), max_amt = max(JudgmentAmt)) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter(grepl("^Responsible", Disposition)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarise(lat=median(lat), long=median(long), num_viols = last(num_viols), 
            num_responsible = last(num_disposition), max_amt =last(max_amt)) %>%
  unique() 

head(bld_list_viol)
```
Adding the violation code counts
```{r}
bld_list_viol %<>%
  left_join(violcodes_counts, by="address")

colnames(bld_list_viol) <- make.names(colnames(bld_list_viol))
head(bld_list_viol)
```


Loading calls to 311, typically complains
```{r}
data_311 <- read_csv(paste(datadir, "detroit-311.csv", sep="/"))

#converting to factors
cols <- c("issue_type", "ticket_status")
data_311 %<>% mutate_each_(funs(factor(.)),cols)

#converting to dates
cols <-c("ticket_closed_date_time", "acknowledged_at", "ticket_created_date_time",
         "ticket_last_updated_date_time")
data_311 %<>% mutate_each_(funs(parse_date_time(.,orders="mdY HMS Op",tz="America/Detroit")),cols)

#dplyr::glimpse(data_311)
summary(data_311)
```
There are 23 types of 311 issues, we could extract counts for each
```{r}
types311 <- data_311 %>%
  group_by(issue_type) %>%
  summarise(num_311type = n()) %>%
  arrange(desc(num_311type))
types311
```

Generating features for each issue type as a count per address
```{r}
type311counts <- data_311 %>%
  select(address, issue_type) %>%
  group_by(address, issue_type) %>%
  summarize(num_type311 = n()) %>%
  ungroup() %>%
  spread(issue_type, num_type311, fill = 0) 

head(type311counts)
```

Most tickets are either archived or closed, probably there is no discrimination around that
```{r}
data_311 %>%
  group_by(ticket_status) %>%
  summarise(num_tx_status = n()) %>%
  arrange(desc(num_tx_status))
```
Rating could be interesting to use
```{r}
data_311 %>%
  group_by(rating) %>%
  summarise(num_rating = n()) %>%
  arrange(desc(num_rating))
```

All entries are for city of Detroit, which is good, but won't be discriminating
```{r}
data_311 %>%
  group_by(city) %>%
  summarise(num_city = n()) %>%
  arrange(desc(num_city))
```

Grouping by address, it seems there are multiple lat/longs per address, removing those entries that have a lat/long STDEV larger than 10e-4 (~11m)
```{r}
bld_list_311 <- data_311 %>%
  select(address, lat, long=lng, rating, type311 = issue_type, rating) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarize(lat=median(lat), long=median(long), num_311 = n(), max_rating = max(rating), min_rating=min(rating), diff_rating = max(rating) - min(rating)) %>%
  unique()

head(bld_list_311)
```
Most 311 entries don't change the rating
```{r}
bld_list_311 %>%
  group_by(diff_rating) %>%
  tally(sort=TRUE)
```

Adding the 311 issue type counts
```{r}
bld_list_311 %<>%
  left_join(type311counts, by="address")

colnames(bld_list_311) <- make.names(colnames(bld_list_311))
head(bld_list_311)
```

Loading criminal incidents in Detroit
```{r}
data_crime <- read_csv(paste(datadir, "detroit-crime.csv", sep="/"))

#converting to factors
cols <- c("CATEGORY","STATEOFFENSEFILECLASS","PRECINCT","COUNCIL","NEIGHBORHOOD")
data_crime %<>% mutate_each_(funs(factor(.)),cols)

#converting to dates
cols <-c("INCIDENTDATE")
data_crime %<>% mutate_each_(funs(parse_date_time(.,orders="mdY HMS Op",tz="America/Detroit")),cols)

#dplyr::glimpse(data_crime)
summary(data_crime)
```
There are 50 categories of crimes, could be added as counts to the features
```{r}
crime_categories <- data_crime %>%
  group_by(CATEGORY) %>%
  tally(sort=TRUE)

crime_categories
```

Seems that crime is spread evenly, except for districts 3 and 5
```{r}
data_crime %>% 
  group_by(COUNCIL) %>%
  tally(sort=TRUE)
```

Grouping by address, it seems there are multiple lat/longs per address, and about 1000 entries have a lat/long STDEV larger than 10e-4 (~11m distance), so I removed those
```{r}
bld_list_crime <- data_crime %>%
  select(address=ADDRESS, lat=LAT, long=LON, typecrime = CATEGORY) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  filter(long > -83.3 && long < -82.8) %>%
  filter(lat > 42.2 && lat < 42.5) %>%
  summarize(lat=median(lat), long=median(long), num_crime = n()) %>%
  unique()

head(bld_list_crime)
```

Generating features for each issue type as a count per address
```{r}
crime_category_counts <- data_crime %>%
  select(address=ADDRESS, CATEGORY) %>%
  group_by(address, CATEGORY) %>%
  summarize(num_category = n()) %>%
  ungroup() %>%
  spread(CATEGORY, num_category, fill = 0) 

head(crime_category_counts)
```

Adding the 311 issue type counts
```{r}
bld_list_crime %<>%
  left_join(crime_category_counts, by="address")

colnames(bld_list_crime) <- make.names(colnames(bld_list_crime))
head(bld_list_crime)
```

Let's join all lat/longs and reduce their precision to 4 digits (~11m accuracy) REF: https://en.wikipedia.org/wiki/Decimal_degrees#Precision
```{r}
bld_list_coord <- bld_list_311 %>%
  select(lat, long) %>%
  bind_rows(select(bld_list_crime, lat, long)) %>%
  bind_rows(select(bld_list_viol, lat, long)) %>%
  transmute(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  distinct() %>%
  arrange(desc(lat), desc(long))
  
bld_list_coord
```
```{r}
lt <- 42.4876
lg <- -82.8972
e <- 0.001

bld_list_coord %>%
  filter(abs(lat-lt)<e)
```


Let's reduce the precision to 4 decimal points (~11m in the equator)
```{r}
bld_list_311 %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))

bld_list_crime %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))

bld_list_viol %<>%
  mutate(lat = round(lat,digits=4), long = round(long,digits=4)) %>%
  mutate(coord = paste(lat,long)) %>%
  unique() %>%
  arrange(desc(lat), desc(long))
```

Direct join by lat/long coordinates at 4 digit precision (~11m)
```{r}
### NOT doing geo_join, it's too slow!
#bld_list_crime_311_geosmart <- bld_list_crime %>% 
#  head(100) %>%
#  geo_join(bld_list_311, by=c("lat", "long"),  max_dist=0.1, unit="km", mode="inner")

bld_list_crime_311_viol <- bld_list_crime %>%
  full_join(bld_list_311, by = "coord") %>%
  full_join(bld_list_viol, by = "coord") %>% 
  select(-address.x, -lat.x, -long.x, -address.y, -lat.y, -long.y, 
       -address, -lat, -long)

bld_list_crime_311_viol %<>% separate(coord, c("lat","long"), sep=" ", remove=FALSE) %>%
  mutate(lat = as.double(lat), long = as.double(long))

bld_list_crime_311_viol %<>% filter(! is.na(lat))
```

Adding surrounding statistics, 0.001 ~ 111m
```{r}
bld_list_crime_311_viol %<>% 
  mutate(coord_neigh = paste(round(lat,digits=3),round(lat,digits=3))) %>%
  group_by(coord_neigh) %>%
  mutate(
    num_crime_neigh = sum(num_crime),
    num_311_neigh = sum(num_311),
    max_rating_neigh = max(max_rating),
    min_rating_neigh = min(min_rating),
    num_viols_neigh = sum(num_viols),
    num_respons_neigh = sum(num_responsible),
    avg_max_amt_neigh = mean(max_amt)
  ) %>%
  ungroup()
```


Taking care of NA's, doing some dummy imputation
```{r}
codesviol <- make.names(unlist(lapply(unique(violCodes_manual_categorization$ViolGroup), as.character)))
codes311 <- make.names(unlist(lapply(types311$issue_type, as.character)))
codescrime <- make.names(unlist(lapply(crime_categories$CATEGORY, as.character)))

cols <- c("num_crime", "num_311", "num_viols", "num_responsible",
          "num_crime_neigh", "num_311_neigh", "num_viols_neigh", "num_respons_neigh")
cols <- c(cols, codesviol, codes311, codescrime)
bld_list_crime_311_viol %<>% 
  mutate_each_(funs(ifelse(is.na(.),0,.)),cols)

cols <- c("max_rating", "min_rating", "diff_rating", "max_amt", 
          "max_rating_neigh", "min_rating_neigh", "avg_max_amt_neigh")
bld_list_crime_311_viol %<>% mutate_each_(funs(ifelse(is.na(.),-1,.)),cols)
```


Speeding up version by looking computing distance directly in degrees, 0.0001 ~ 11m ~ 37ft
```{r}
in_blight <- function(lt, ln, data) {
  data %>% 
    filter(sqrt((lat-lt)^2+(long-ln)^2)<rdegr+0.0001) %>%
    nrow()
}

indata_degrees <- indata %>%
  mutate(rdegr = 0.0001/37 * r)
```


The mashup results in about 8523 blighted entries, out of 144k total records. 2k blighted entries are potentially overlaps due to lat/long matching
```{r}
modeling_data <- bld_list_crime_311_viol 

modeling_data %<>%
  rowwise() %>%
  mutate(nblighted = in_blight(lat, long, indata_degrees)) 

modeling_data %>%
  filter(nblighted > 0) %>% 
  nrow()
```

Creating dependent variable "condition" as boolean from "nblighted" (this describes the number of records blighted in the same lat/long)
```{r}
modeling_data %<>%
  mutate(condition = factor(if_else(nblighted > 0, "BLIGHTED", "NOT_BLIGHTED"))) %>%
  select(-coord, -lat, -long, -nblighted, -coord_neigh)

table(modeling_data$condition)
```
Balancing the two classes by downsampling the number of cases of non-blight (not ideal, but helps modeling and compute time)
```{r}
mod_data_sampled <- modeling_data %>%
  filter(condition == "BLIGHTED")

mod_data_sampled <- modeling_data %>%
  filter(condition == "NOT_BLIGHTED") %>% 
  sample_n(9000) %>%
  bind_rows(mod_data_sampled)

table(mod_data_sampled$condition)
```

Creating a set-aside dataset for test, and a modeling set
```{r}
set.seed(107)
inTest <- createDataPartition(y=mod_data_sampled$condition, p=0.1, list=FALSE)
testSet <- mod_data_sampled[inTest,]
modelSet <- mod_data_sampled[-inTest,]
table(testSet$condition)
```
Creating a training and validation data sets for model training and validation, the training set is about 20k records
```{r}
set.seed(107)
inValid <- createDataPartition(y=modelSet$condition, p=0.15, list=FALSE)
trainSet <- mod_data_sampled[-inValid,]
validSet <- mod_data_sampled[inValid,]
table(trainSet$condition)
```
As for the validation set, it is about 3k records
```{r}
table(validSet$condition)
```
Plotting the histogram of number of violations for both blighted and not blighted buildings, we see some difference that might contribute to discriminate the 2 classes
```{r}
p <- figure(width = 600, height = 350) %>% 
  ly_hist(num_viols, data = trainSet[which(trainSet$condition == "NOT_BLIGHTED"),], color = "blue", alpha = 0.25, freq = F, breaks = 25) %>%
  ly_hist(num_viols, data = trainSet[which(trainSet$condition == "BLIGHTED"),], color = "red", alpha = 0.25, freq = F, breaks = 25) 
p
```
Let's train a general linear model (GLM) with 5-fold cross-validation trying to predict the bligth condition only by the number of violations reported in the coordinate, which yields 67.6% AUC on the ROC, 76% true positive, and 43% true negative. 
```{r}
# 5 fold cross-validation, 1 repetition
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 1,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )

# general linear model
glmModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "glm",
                  trControl = ctrl,
                  metric = "ROC"
                  )
glmModel
```
Let's check the results in the validation set
```{r}
glmVal <- predict(glmModel, newdata = validSet)
confusionMatrix(glmVal, validSet$condition, positive = "BLIGHTED")
```

A decision tree has a similar performance, 67% AUC on ROC for cross-validation
```{r}
# decision tree model
dtreeModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "rpart",
                  trControl = ctrl,
                  metric = "ROC"
                  )
dtreeModel
```

Let's check the results in the validation set
```{r}
dtreeVal <- predict(dtreeModel, newdata = validSet)
confusionMatrix(dtreeVal, validSet$condition, positive = "BLIGHTED")
```

A random forest doesn't yield significantly different results, with an 66.3% AUC on ROC
```{r}
# random forest model
rfModel <- train(condition ~ num_viols,
                  data = trainSet,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel
```
Let's check the results in the training set
```{r}
rfVal <- predict(rfModel, newdata = trainSet)
confusionMatrix(rfVal, trainSet$condition, positive = "BLIGHTED")
```

Let's check the results in the validation set
```{r}
rfVal <- predict(rfModel, newdata = validSet)
confusionMatrix(rfVal, validSet$condition, positive = "BLIGHTED")
```
Comparing the models across cross-validation resamples, we see a similar performance in terms of ROC. 
```{r}
resamps <- resamples(list(glm = glmModel, dtree = dtreeModel, rf = rfModel))
summary(resamps)
```

The fact that the random forest performance in training and validation sets was similar, eludes that the model in not complex enough. Let's add all the extracted features above in the dtree (about 101).
```{r}
# decision tree model
dtreeModel2 <- train(condition ~ .,
                  data = trainSet,
                  method = "rpart",
                  trControl = ctrl,
                  metric = "ROC",
                  na.action=na.exclude
                  )
dtreeModel2
```

The results in cross-validation didn't show a significant improvement. The validation set also doesn't show major differences.
```{r}
dtree2Val <- predict(dtreeModel2, newdata = validSet)
confusionMatrix(dtree2Val, validSet$condition, positive = "BLIGHTED")
```
Let's try a random forest with all the variables
```{r}
# random forest model
rfModel2 <- train(condition ~ .,
                  data = trainSet,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel2
```
```{r}
rf2Val <- predict(rfModel2, newdata = validSet)
confusionMatrix(rf2Val, validSet$condition, positive = "BLIGHTED")
```
```{r}
rf2varImp <- varImp(rfModel2, scale = FALSE)
plot(rf2varImp, top=28)
```
```{r}
trainSet2 <- trainSet %>% select(max_amt, num_viols, num_responsible, num_crime, X9.1.36.a., X22.2.88, min_rating, X9.1.81.a., diff_rating, num_311, max_rating, X22.2.88.b., X9.1.43.a.....Dwelling., condition)
# random forest model
rfModel3 <- train(condition ~ .,
                  data = trainSet2,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel3
```

```{r}
table(trainSet$num_crime>0)
table(trainSet$num_viols>0)
table(trainSet$num_311>0)
table(bld_list_crime_311_viol$num_311>0)
#bld_list_crime_311_viol %<>% mutate(num_viols = as.integer(num_viols), num_crime = as.integer(num_crime), num_311 = as.integer(num_311))
bld_list_crime_311_viol %>% filter(num_viols>-1) %>% filter(num_crime>0) %>% filter(num_311>-1)
summary(bld_list_crime_311_viol)
```
Looking at the rate of blight by violation code, "X9.1.45" and "X9.1.43.a.....Dwelling." (over 75%). The former being "Failure of owner to comply with an emergency or imminent danger order of one- or two-family dwelling or building" and "Failure of owner of one- or two-family dwelling to comply with an emergency or imminent danger order concerining an unsafe or unsanitary structure or unlawful occupancy"
```{r}
count_violtype_blighted <- trainSet %>% 
  filter(condition == "BLIGHTED") %>%
  select(starts_with("X")) %>%
  colSums()

count_violtype_all <- trainSet %>% 
  select(starts_with("X")) %>%
  colSums()

blightrate_by_violtype <- count_violtype_blighted / count_violtype_all
blightrate_by_violtype
```


Next steps:
- why 311 doesn't have much effect? Maybe not enough representation? 
  - How many 311 records are in modeling data? It's the smallest set (13k records)
  - And how about violations & crime? Violations had the most records, 73k, crime 56k
  - Train ensuring the overlap data (inner join), is in the model set
- investigate crime/311/viol type counts as features (like bag of words)
- 311: investigate effect of time (e.g., between created and closed, or # tx / unit of time)

Other data:
- MLS / Zillow (last time sold, price, # sold houses around)
- Detroit Parcel data