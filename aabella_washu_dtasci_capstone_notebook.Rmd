---
title: 'Coursera WashU Dtasci Capstone Project: BLIGHT'
always_allow_html: yes
output:
  html_notebook: default
---

Loading libraries
```{r, include=FALSE}
library(readr)
library(magrittr)
library(dplyr)
#install.packages("lucr")
library(lucr)
#install.packages("lubridate")
library(lubridate)
#install.packages("rbokeh")
library(rbokeh)
#install.packages("geosphere")
library(geosphere)
#install.packages("caret")
library(caret)
```

Setting data location, make sure the files below are available so the rest runs
```{r}
datadir <- "../data"
list.files(datadir)
```

Loading data permits -- mainly blight incidents
```{r}
data_permits <- read_tsv(paste(datadir, "detroit-demolition-permits.tsv", sep="/"))
head(data_permits)
```

Data permits exploration
```{r}
#converting to factors
cols <- c("CASE_TYPE", "CASE_DESCRIPTION", "LEGAL_USE", "BLD_PERMIT_TYPE",
          "PERMIT_DESCRIPTION", "BLD_PERMIT_DESC", "BLD_TYPE_USE", "RESIDENTIAL",
          "DESCRIPTION", "BLD_TYPE_CONST_COD", "BLD_ZONING_DIST", "BLD_USE_GROUP",
          "BLD_BASEMENT", "FEE_TYPE", "CSF_CREATED_BY","CONDITION_FOR_APPROVAL")
data_permits %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("PCF_AMT_PD", "PCF_AMT_DUE", "PCF_UPDATED","ESTIMATED_COST")
data_permits %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("PERMIT_APPLIED","PERMIT_ISSUED","PERMIT_EXPIRES")
# to use lubridate, need to figure it out -- 
data_permits %<>% mutate_each_(funs(parse_date_time(.,orders="mdy",tz="America/Detroit")),cols)
summary(data_permits)
```
Extracting building lat longs
```{r}
data_permits %<>%
  #filter out permits that have no lat/long
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",site_location)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", site_location))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", site_location))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", site_location))

```

Create a list of buildings doing some magic to remove those entries whose lat/long stdev is larger than 10e-4 (~11m). Not much gets removed actually, but it's consistent with steps below. Removed those records whose address was a lat/long (only ~40).
```{r}
bld_list_permit <- data_permits %>%
  mutate(r = sqrt(PARCEL_SIZE/pi) ) %>%
  select(address=address_only, PARCEL_NO, LOT_NUMBER, PERMIT_ISSUED, PARCEL_SIZE, lat, long, r) %>%
  filter(! grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",address)) %>%
  arrange(address, desc(PERMIT_ISSUED)) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  summarise(n_permits=n(), last_permit=first(PERMIT_ISSUED), 
            lat=mean(lat), long=mean(long), r=last(r))
head(bld_list_permit)
```

Plot lat/longs to a map
- centering to Detroit 42.3314 N, 83.0458 W
- more about rbokeh: http://hafen.github.io/rbokeh/rd.html#gmap
- lots of cool maps: https://snazzymaps.com 
- Available styles are: “subtle_grayscale”, “shades_of_grey”, “blue_water”, “pale_dawn”, “blue_essence”, “apple_mapsesque”, “midnight_commander”, “light_monochrome”, “paper”, “retro”, “flat_map”, “cool_grey”
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = bld_list_permit, hover = c(address, r), 
            col = 'purple', alpha = 0.1) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```

Testing GeoSphere package to calculate distance between 2 lat/long coordinates
```{r}
#this should be around 1160ft according to Google Maps
#https://www.google.com/maps/dir/42.3655979,-71.1040001/42.3683169,-71.1017685/@42.3668028,-71.10503,17z/data=!4m2!4m1!3e2
m_in_ft <- 0.3048
distGeo(c(-71.1040001,42.3655979), c(-71.1017685,42.3683169))/m_in_ft
```

Create a function to output the buildings that are within proximity of lat/lon coordinates
```{r}
lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]
rangeft <- 1500
bld_list_permit %>% 
#  mutate(dt = distGeo(c(ln,lt), c(long, lat))/m_in_ft) %>%
  filter(!is.na(r)) %>%
  rowwise() %>%
  mutate(dt = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
  filter(dt <= rangeft + r) %>%
  head(10)

#distGeo(c(ln,lt),c(bld_list$long,bld_list$lat))
```
Function to return number of buildings around a particular lat long within a predefined range (by default 500m or 1640ft)
- input data, for simplicity, should have 4 fields
  - PARCEL_NO: assumed it's a unique identifier for building)
  - lat: building lat coordinate
  - long: building long coordinate
  - r: estimated building's area radious (when area is approximated to circle)
- the other input variables are:
  - lt: target lat coordinate
  - ln: target long coordinate
  - rangeft (optional, range in feet to look buildings around target lat/long)
  - m_in_ft (option)
```{r}
bld_in_range <- function(lt, ln, data, rangeft = 1640, m_in_ft = 0.3048) {
  data %>% 
    #filter(!is.na(r)) %>%
    rowwise() %>%
    mutate(dist_ft = distGeo(c(ln,lt),c(long,lat))/m_in_ft) %>%
    filter(dist_ft <= rangeft + r) %>%
    #select(address, dist_ft) %>%
    #arrange(dist_ft)
    nrow()
}

indata <- bld_list_permit %>% 
  filter(!is.na(r)) %>%
  unique() %>%
  select(address, lat, long, r)

lt <- bld_list_permit$lat[3]
ln <- bld_list_permit$long[3]

bld_in_range(lt, ln, indata, rangeft = 500)
```
Loading blight violation incidents
```{r}
data_violations <- read_csv(paste(datadir, "detroit-blight-violations.csv", sep="/"))
head(data_violations)
```

Data Violation exploration
```{r}
#converting to factors
cols <- c("AgencyName","ViolationCode","Disposition","PaymentStatus","Void",
          "ViolationCategory","Country")
data_violations %<>% mutate_each_(funs(factor(.)),cols)

#converting $$ to numeric
cols <- c("FineAmt","AdminFee","LateFee","StateFee","CleanUpCost","JudgmentAmt")
data_violations %<>% mutate_each_(funs(from_currency(.)),cols)

#converting to dates
cols <-c("TicketIssuedDT","HearingDT")
# to use lubridate, need to figure it out -- 
#data_violations %<>% mutate_each_(funs(from_currency(.)),cols)
summary(data_violations)
#dplyr::glimpse(data_violations)
```

Getting the violation codes
Need to categorize them
```{r}
violCodes <- data_violations %>% 
  select(ViolationCode, ViolDescription) %>%
  unique()
```

Generated lat/longs and address, and cleaned data
 1) Detroit addresses only, 
 2) removed Disposition "not responsible" and "PENDING JUDGEMENT" as might not be a violation
 Didn't filter by country as it removed most of the entries (from 300k to 13k!!)
```{r}
viol_list <- data_violations %>% 
#  filter(Country == "US") %>%
  filter(grepl("\\([0-9\\.\\-]+, *[0-9\\.\\-]+\\)",ViolationAddress)) %>%
  #extracting lat longs
  mutate(lat = as.double(sub(".*\\(([0-9\\.\\-]+),.*","\\1", ViolationAddress))) %>%
  mutate(long = as.double(sub(".*, *([0-9\\.\\-]+).*","\\1", ViolationAddress))) %>%
  mutate(address_only = sub("([^\\(]+)\\([0-9\\.\\-]+,.*","\\1", ViolationAddress)) %>%
  filter(grepl("Detroit",ViolationAddress)) %>%
  filter(! grepl("Not responsible",Disposition)) %>%
  filter(! grepl("PENDING", Disposition)) %>%
  select(lat, long, ViolationCode, Disposition, JudgmentAmt, PaymentStatus, ViolationCategory, address_only) 

head(viol_list)
```

What happens with Disposition?
```{r}
viol_list %>%
  select(Disposition) %>%
  group_by(Disposition) %>%
  summarise(n())
```

There are about 80k unique lat/longs, some of them generate a huge amount of violations, here are the top 30 lat/longs
```{r}
top30viols <- viol_list %>%
  select(lat, long) %>%
  mutate(geocord = paste(lat,long)) %>%
  group_by(geocord) %>%
  summarize(lat=last(lat), long=last(long), num_viols_in_geo = n()) %>%
  arrange(desc(num_viols_in_geo)) %>%
  head(30)
top30viols
```

Plotting them, we see that the top one has 13k violations and it's in the center of Detroit, probably a standard lat/long coordinate when not the actual is not available. I'm not sure about the others with over 1000 violations... 
```{r}
p <- gmap(lat = 42.37, lng = -83.10, zoom = 11, width = 600, height = 350,
          map_style = gmap_style("apple_mapsesque")) %>%
  ly_points(long, lat, data = top30viols, hover = num_viols_in_geo, 
            col = 'red', alpha = pmin(num_viols_in_geo / 1000, 1)) %>%
  x_axis(visible = FALSE) %>%
  y_axis(visible = FALSE)
p
```


Are there the same number of unique addresses?
```{r}
viol_list %>%
  select(address_only) %>%
  group_by(address_only) %>%
  summarize(num_viols_in_address = n()) %>%
  arrange(desc(num_viols_in_address))
```

Well, it seems that there are about 85k unique addresses, and 73k unique lat/longs. Let's see how they contrast in terms of number of violations (lat/long vs. address)
```{r}
viol_list_cleaned <- viol_list %>%
  mutate(geocoord = paste(lat,long)) %>%
  group_by(geocoord) %>%
  mutate(num_viols_in_geocoord = n())

viol_list_cleaned %<>% 
  group_by(address_only) %>%
  mutate(num_viols_in_address = n())

head(viol_list_cleaned)
```

It seems that about 26k entries have different number of violations when looking by address or lat/long, so getting rid of them
```{r}
viol_list_cleaned %<>%
  filter(! num_viols_in_address != num_viols_in_geocoord) %>%
  group_by(ViolationCode, address_only) %>%
  mutate(num_viols_by_vcode = n()) %>%
  arrange(desc(num_viols_in_geocoord)) %>%
  unique()

head(viol_list_cleaned)
```

Getting a list of buildings, the grouping wouldn't be necessary as there is a one to one lat/long to address correspondance, but leaving it just in case. 
```{r}
bld_list_viol <- viol_list_cleaned %>%
  ungroup() %>%
  select(address=address_only, lat, long) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  summarize(lat=mean(lat), long=mean(long)) %>%
  select(address, lat, long) %>%
  unique()

head(bld_list_viol)
```

Loading calls to 311, typically complains
```{r}
data_311 <- read_csv(paste(datadir, "detroit-311.csv", sep="/"))
head(data_311)
```

Grouping by address, it seems there are multiple lat/longs per address, removing those entries that have a lat/long STDEV larger than 10e-4 (~11m)
```{r}
bld_list_311 <- data_311 %>%
  select(address, lat, long=lng) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
  summarize(lat=mean(lat), long=mean(long)) %>%
  select(address, lat, long) %>%
  unique()

head(bld_list_311)
```


Loading criminal incidents in Detroit
```{r}
data_crime <- read_csv(paste(datadir, "detroit-crime.csv", sep="/"))
head(data_crime)
```

Grouping by address, it seems there are multiple lat/longs per address, and about 1000 entries have a lat/long STDEV larger than 10e-4 (~11m distance), so I removed those
```{r}
bld_list_crime <- data_crime %>%
  select(address=ADDRESS, lat=LAT, long=LON) %>%
  group_by(address) %>%
  mutate(sdlat=sd(lat), sdlong=sd(long)) %>%
  filter((sdlat<10e-4 && sdlong<10e-4) || (is.na(sdlat) && is.na(sdlong))) %>%
#  filter(!is.na(sdlat)) %>%
#  group_by(address) %>%
#  summarize(newsdlat=sd(lat),newsdlong=sd(long)) %>%
#  select(newsdlat, newsdlong) %>%
#  summarize(mean(newsdlat), mean(newsdlong))
  summarize(lat=mean(lat), long=mean(long)) %>%
  select(address, lat, long) %>%
  unique()

head(bld_list_crime)
```


```{r}
bld_list_all <- bld_list_viol %>%
  bind_rows(bld_list_311) %>%
  bind_rows(bld_list_crime) %>%
  select(lat, long) %>%
  unique()
```

Speeding up version by looking computing distance directly in degrees, 2*10e-4 ~ 750 feet
0.001 lat = 608 feet, 0.0001 lat = 60.8 feet
0.001 long = 448 feet, 0.0001 long = 44.8 feet
```{r}
in_blight <- function(lt, ln, data) {
  data %>% 
#   filter((abs(lat-lt)+abs(long-ln))<2*10e-5) %>%
    filter(sqrt((lat-lt)^2+(long-ln)^2)<rdegr+10e-5) %>%
    nrow()
}

#in_blight(lt, ln, indata)
#bld_in_range(lt, ln, indata)
#in_blight(lt, ln)
indata_degrees <- indata %>%
  mutate(rdegr = 0.0001/60.8 * r)

bld_list_all_ref <- bld_list_all %>%
#  head(100) %>%
  rowwise() %>%
  mutate(nblighted = in_blight(lat, long, indata_degrees))
#  mutate(nblighted = in_blight(lat, long))
  
bld_list_all_ref
```
See how many are bligthed
```{r}
bld_list_all_ref %>%
  filter(nblighted>0) %>%
  nrow()
```
Going through the blight violation records, we can find 11,492 entries that belong to a blighted building, out of a 138,367 records in total (~10%)
```{r}
modeling_data <- viol_list_cleaned %>%
  group_by(geocoord) %>%
  summarise(n_viols = n(),lat = last(lat), long = last(long))
  
modeling_data %<>%
  rowwise() %>%
  mutate(nblighted = in_blight(lat, long, indata_degrees)) 

modeling_data %>%
  filter(nblighted > 0) %>% 
  nrow()
```

Creating dependent variable "condition" as boolean from "nblighted" (this describes the number of records blighted in the same lat/long)
```{r}
modeling_data %<>%
  mutate(condition = factor(if_else(nblighted > 0, "BLIGHTED", "NOT_BLIGHTED"))) 

table(modeling_data$condition)
```
Balancing the two classes by downsampling the number of cases of non-blight (not ideal, but helps modeling and compute time)
```{r}
mod_data_sampled <- modeling_data %>%
  filter(condition == "BLIGHTED")

mod_data_sampled <- modeling_data %>%
  filter(condition == "NOT_BLIGHTED") %>% 
  sample_n(6000) %>%
  bind_rows(mod_data_sampled)

table(mod_data_sampled$condition)
```
Creating a set-aside dataset for test, and a modeling set
```{r}
set.seed(107)
inTest <- createDataPartition(y=mod_data_sampled$condition, p=0.1, list=FALSE)
testSet <- mod_data_sampled[inTest,]
modelSet <- mod_data_sampled[-inTest,]
table(testSet$condition)
```
Creating a training and validation data sets for model training and validation, the training set is about 20k records
```{r}
set.seed(107)
inValid <- createDataPartition(y=modelSet$condition, p=0.15, list=FALSE)
trainSet <- mod_data_sampled[-inValid,]
validSet <- mod_data_sampled[inValid,]
table(trainSet$condition)
```
As for the validation set, it is about 3k records
```{r}
table(validSet$condition)
```
Ongoing work to plot number of blight violations for each class
```{r}
p <- figure(width = 600, height = 350) %>% 
#  ly_hist(n_viols, data = trainSet, breaks = 100, freq = FALSE) 
  ly_hist(n_viols, data = trainSet, label = "condition", freq = FALSE) 
p
```


Let's train a general linear model (GLM) with 5-fold cross-validation trying to predict the bligth condition only by the number of violations reported in the coordinate ("num_viols_in_geocoord", calculated before). The performance is quite low. 
```{r}
# 5 fold cross-validation, 1 repetition
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     repeats = 1,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )

# general linear model
glmModel <- train(condition ~ n_viols,
                  data = trainSet,
                  method = "glm",
                  trControl = ctrl,
                  metric = "ROC"
                  )
glmModel
```
Let's check the results in the validation set
```{r}
glmVal <- predict(glmModel, newdata = validSet)
confusionMatrix(glmVal, validSet$condition, positive = "BLIGHTED")
```

Let's try a decision tree
```{r}
# decision tree model
dtreeModel <- train(condition ~ n_viols,
                  data = trainSet,
                  method = "rpart",
                  trControl = ctrl,
                  metric = "ROC"
                  )
dtreeModel
```
Let's check the results in the validation set
```{r}
dtreeVal <- predict(dtreeModel, newdata = validSet)
confusionMatrix(dtreeVal, validSet$condition, positive = "BLIGHTED")
```

Let's try a random forest
```{r}
# random forest model
rfModel <- train(condition ~ n_viols,
                  data = trainSet,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC"
                  )
rfModel
```
Let's check the results in the validation set
```{r}
rfVal <- predict(rfModel, newdata = validSet)
confusionMatrix(rfVal, validSet$condition, positive = "BLIGHTED")
```
Let's compare the models across cross-validation resamples
```{r}
resamps <- resamples(list(glm = glmModel, dtree = dtreeModel, rf = rfModel))
summary(resamps)
```


